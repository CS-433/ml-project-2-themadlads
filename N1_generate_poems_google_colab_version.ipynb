{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heard-airplane",
   "metadata": {
    "id": "heard-airplane"
   },
   "source": [
    "# Notebook N1 : generate rhyming poems\n",
    "\n",
    "This notebooks allows to generate rhyming poems. First, given a form, it generates a non-rhyming poem correposponding to stage 1 of the CRPO system, and then, given a rhyming pattern it re-generates last words in order for it to rhyme, corresponding to stage 4.\n",
    "\n",
    "- Make sure to define the variables you want in \"0. Arguments\".\n",
    "- Sections 1-4 are just funtion definitions.\n",
    "- In Section 5 you call the functions: load and build the model, generate non-rhyming poem, generate rhyming poem out of previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OiELutID9KRf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiELutID9KRf",
    "outputId": "6853acd6-89fd-41a0-bd9f-47800d0eaac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MbKbklqiXu5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbKbklqiXu5a",
    "outputId": "e2a36bd2-b350-4187-acdf-5998391f3dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 38.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 60.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 60.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69v7gH3gBgv7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69v7gH3gBgv7",
    "outputId": "dcc059b3-f96f-4a20-95df-e94e8ec05891"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-christopher",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "infectious-christopher",
    "outputId": "b29a6b8d-95ea-4ba3-9163-e568c0987c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 37.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pickle5: filename=pickle5-0.0.11-cp38-cp38-linux_x86_64.whl size=236293 sha256=7ad3378bff9f0ebfe6a6381c0f63a16ae0bd62b02783a13c11b5fbdffdd8d78a\n",
      "  Stored in directory: /root/.cache/pip/wheels/25/d4/61/dbd8edd1a0d656be7b4267c85db3b61951eb60016a0154a122\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "from calendar import c\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from string import punctuation\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "!pip3 install pickle5\n",
    "import pickle5 as pickle\n",
    "import difflib\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "#from content.drive.MyDrive.crpo.crpo-notebooks.correct_with_dictionary import postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BBfRsffc-gLZ",
   "metadata": {
    "id": "BBfRsffc-gLZ"
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "def postprocessing (text, punctuation):\n",
    "    \"\"\"\n",
    "    Does a bit of aesthetic postprocessing on the text. Each part is separate and ad hoc quick patch atm, so\n",
    "    things could be improved, added, or deleted - there's lots of redundancies (should be cleaned up).\n",
    "    \"\"\"\n",
    "\n",
    "    # Don't start with punctuation or ' '.\n",
    "    while text[0] in punctuation:\n",
    "        text = text[1:]\n",
    "\n",
    "    # Punctuation immediately follows alphabetical characters (no ' ' in between).\n",
    "    idxs_punctuation = []\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in punctuation[:-1]:\n",
    "            if text[i-1] == ' ':\n",
    "                idxs_punctuation += [i - 1]\n",
    "    text = [text[i] for i in range(len(text)) if i not in idxs_punctuation]\n",
    "\n",
    "    # Don't have two punctuation signs next to each other.\n",
    "    idxs_punctuation = []\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in punctuation:\n",
    "            if text[i+1] in punctuation:\n",
    "                idxs_punctuation += [i + 1]\n",
    "    text = [text[i] for i in range(len(text)) if i not in idxs_punctuation]\n",
    "\n",
    "    # Add ' ' after punctuation.\n",
    "    new_text = []\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] not in punctuation:\n",
    "            new_text += [text[i]]\n",
    "        elif text[i] == ' ':\n",
    "            new_text += [text[i]]\n",
    "        elif text[i] in punctuation[:-1]:\n",
    "            new_text += [text[i], ' ']\n",
    "    text = new_text\n",
    "\n",
    "    # No ' ' after ' ', '↵' into '\\n', no verse start with ' ', write '...' correctly.\n",
    "    text_joined = ''.join(text)\n",
    "    text_joined = re.sub(r\" +\", ' ', text_joined)\n",
    "    text_joined = re.sub(r\"↵\", '\\n', text_joined)\n",
    "    text_joined = re.sub(r'\\n ','\\n', text_joined)\n",
    "    text_joined = re.sub('\\.\\.+', '...', text_joined)\n",
    "\n",
    "    # Correct uppercase and lowercase #The new gpt-2 model doesn't generally makes casing mistakes \n",
    "    #text = [i for i in text_joined]\n",
    "    #text[0] = text[0].upper()\n",
    "    #for i in range(len(text)):\n",
    "        #if text[i-1] in ['.', '!', '?']:\n",
    "            #if text[i] == ' ' and i+1 < len(text):\n",
    "                #text[i+1] = text[i+1].upper()\n",
    "            #else:\n",
    "                #text[i] = text[i].upper()\n",
    "        #elif text[i-1] in [',', ':', ';']:\n",
    "            #text[i] = text[i].lower()\n",
    "\n",
    "    # Delete some t percent of ending soft punctuation to make it look a bit more natural.\n",
    "    t = 0.25\n",
    "    idxs_to_delete = []\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '\\n':\n",
    "            if text[i-1] != '\\n':\n",
    "                end = text[i-2]\n",
    "                if end in [',', ':', ';']:\n",
    "                    if random.random() < t:\n",
    "                        idxs_to_delete += [i-2]\n",
    "    text = [text[i] for i in range(len(text)) if i not in idxs_to_delete]\n",
    "\n",
    "    # Add ' ' before certain punctuation signs.  \n",
    "    #new_text = []\n",
    "    #for i in range(len(text) - 1):\n",
    "        #if text[i+1] not in [':', ';', '!', '?']:\n",
    "            #new_text += [text[i]]\n",
    "        #else:\n",
    "            #new_text +=[text[i], ' ']\n",
    "    #new_text += text[-1]\n",
    "    #text = new_text\n",
    "\n",
    "    # No ' ' before '\\n' and end strophes with punctuation\n",
    "    re_sub = re.sub(r' \\n', '\\n', ''.join(text))\n",
    "    text = [i for i in re_sub]\n",
    "    new_text = []\n",
    "    for i in range(len(text) - 2):\n",
    "        if text[i+1] == '\\n' and text[i+2] == '\\n':\n",
    "            if text[i] in ['!', '?', '.']: #While I don't like the idea of random hard punctuation, I think it's better than nothing.\n",
    "                new_text += [text[i]]\n",
    "            elif text[i] in [';', ',', ':']: \n",
    "                new_text += [text[i]] #But not better than soft punctuation.\n",
    "            else:\n",
    "                new_text += [text[i], random.choice(['!', '?', '.'])]\n",
    "        else:\n",
    "            new_text += [text[i]]\n",
    "    new_text += text[-2:]\n",
    "    text = new_text\n",
    "\n",
    "    # End with hard punctuation.\n",
    "    if  text[-2] in punctuation and text[-2] not in ['!', '?', '.']:\n",
    "        text[-2] = random.choice(['!', '?', '.'])\n",
    "        text[-1] = \"\"\n",
    "    elif text[-2] in ['!', '?', '.']:\n",
    "        text[-1] = \"\"\n",
    "    else:\n",
    "        text[-1] = random.choice(['!', '?', '.'])\n",
    "\n",
    "    return ''.join(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-montana",
   "metadata": {
    "id": "loose-montana"
   },
   "source": [
    "## 0. Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MLj3POEe_OU3",
   "metadata": {
    "id": "MLj3POEe_OU3"
   },
   "outputs": [],
   "source": [
    "# -1 for CPU, 0 to n for CUDA devices (like a NIVDIA GPU)\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-brighton",
   "metadata": {
    "id": "numerical-brighton"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIRECTORY_FILE = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-assist",
   "metadata": {
    "id": "opposite-assist"
   },
   "outputs": [],
   "source": [
    "# Define form\n",
    "# approximations for sonnet\n",
    "s = 4\n",
    "sonnet_l = 10 #should be 12, but let's speed up generation\n",
    "HAIKU = [[7*s, 9*s, 7*s], [8*s, 11*s, 8*s]]\n",
    "SONNET = [[sonnet_l*s]*4, [sonnet_l*s]*4, [sonnet_l*s]*3, [sonnet_l*s]*3]\n",
    "QUATRAIN = [[10*s]*4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-syndrome",
   "metadata": {
    "id": "israeli-syndrome"
   },
   "outputs": [],
   "source": [
    "# RIME_PICKLE_FILE = '../utils/data/cons_rime_eng.pickle'\n",
    "RIME_PICKLE_FILE = \"/content/drive/MyDrive/crpo/crpo-notebooks/rhyming_dictionaries.pickle\" # updated phonetic dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-vehicle",
   "metadata": {
    "id": "together-vehicle"
   },
   "outputs": [],
   "source": [
    "RHYMING_SCHEME = \"AABB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-movement",
   "metadata": {
    "id": "sticky-movement",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Load & Build Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-option",
   "metadata": {
    "id": "configured-option"
   },
   "outputs": [],
   "source": [
    "def load_models (models, path):\n",
    "    \"\"\"\n",
    "    Loads and builds the trained language models, their vocab and indices, given path and models' names.\n",
    "    \"\"\"\n",
    "    built_models = []\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    model_path = os.path.join(path, models[0])\n",
    "    model = pipeline('text-generation', model=model_path, tokenizer='gpt2', device = DEVICE, pad_token_id=tokenizer.eos_token_id)\n",
    "    built_models.append(model)\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    built_models.append(model)\n",
    "\n",
    "    built_models.append(tokenizer)\n",
    "\n",
    "    return [built_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-frequency",
   "metadata": {
    "id": "mexican-frequency",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Syllabification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-storm",
   "metadata": {
    "id": "competent-storm"
   },
   "outputs": [],
   "source": [
    "def syllable_count (word) : # Found this algorythm on this website : https://eayd.in/?p=232. Counts the syllables of a word following some rules\n",
    "\n",
    "    word = word.lower()\n",
    "    exception_add = ['serious','crucial']\n",
    "    exception_del = ['fortunately','unfortunately']\n",
    "\n",
    "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
    "    co_two = ['coapt','coed','coinci']\n",
    "\n",
    "    pre_one = ['preach']\n",
    "\n",
    "    syls = 0 #added syllable number\n",
    "    disc = 0 #discarded syllable number\n",
    "\n",
    "    #if letters < 3 : return 1\n",
    "    if len(word) <= 3 :\n",
    "        syls = 1\n",
    "        return syls\n",
    "\n",
    "    #if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
    "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.)\n",
    "\n",
    "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
    "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
    "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
    "                pass\n",
    "            else :\n",
    "                disc+=1\n",
    "\n",
    "    #discard trailing \"e\", except where ending is \"le\"  \n",
    "\n",
    "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while']\n",
    "\n",
    "    if word[-1:] == \"e\" :\n",
    "        if word[-2:] == \"le\" and word not in le_except :\n",
    "            pass\n",
    "\n",
    "        else :\n",
    "            disc+=1\n",
    "\n",
    "    #check if consecutive vowels exists, triplets or pairs, count them as one.\n",
    "\n",
    "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
    "    disc+=doubleAndtripple + tripple\n",
    "\n",
    "    #count remaining vowels in word.\n",
    "    numVowels = len(re.findall(r'[eaoui]',word))\n",
    "\n",
    "    #add one if starts with \"mc\"\n",
    "    if word[:2] == \"mc\" :\n",
    "        syls+=1\n",
    "\n",
    "    #add one if ends with \"y\" but is not surrouned by vowel\n",
    "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
    "        syls +=1\n",
    "\n",
    "    #add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
    "\n",
    "    for i,j in enumerate(word) :\n",
    "        if j == \"y\" :\n",
    "            if (i != 0) and (i != len(word)-1) :\n",
    "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
    "                    syls+=1\n",
    "\n",
    "    #if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one.\n",
    "\n",
    "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
    "        syls+=1\n",
    "\n",
    "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
    "        syls+=1\n",
    "\n",
    "    #10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
    "\n",
    "    if word[-3:] == \"ian\" : \n",
    "    #and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
    "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
    "\n",
    "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
    "            syls+=1\n",
    "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
    "        if word[:6] in pre_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #check for \"-n't\" and cross match with dictionary to add syllable.\n",
    "\n",
    "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"]\n",
    "\n",
    "    if word[-3:] == \"n't\" :\n",
    "        if word in negative :\n",
    "            syls+=1\n",
    "        else :\n",
    "            pass   \n",
    "\n",
    "    #Handling the exceptional words.\n",
    "\n",
    "    if word in exception_del :\n",
    "        disc+=1\n",
    "\n",
    "    if word in exception_add :\n",
    "        syls+=1     \n",
    "\n",
    "    # calculate the output\n",
    "    return numVowels - disc + syls\n",
    "\n",
    "def syllable_count_sentence(sent): # Counts the syllables of a sentence\n",
    "    count = 0\n",
    "    sent = sent.replace(\"\\n\", \"\")\n",
    "    sent_tokens = sent.split(\" \")\n",
    "    for word in sent_tokens:\n",
    "            word = word.strip(punctuation)\n",
    "            if word != \"\":\n",
    "                count += syllable_count(word)\n",
    "    return count\n",
    "\n",
    "def flatten(L):\n",
    "    \"\"\"\n",
    "    Flattens a list.\n",
    "    \"\"\"\n",
    "\n",
    "    return [L] if not isinstance(L, list) else [x for X in L for x in flatten(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-basis",
   "metadata": {
    "id": "rational-basis",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Form Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-church",
   "metadata": {
    "id": "backed-church"
   },
   "outputs": [],
   "source": [
    "def generate(form,\n",
    "            models,\n",
    "            outfile_directory,\n",
    "            temperature= 1.0,\n",
    "            print_into_file=False, \n",
    "            d=False,\n",
    "            user_input=\"\"):\n",
    "   \n",
    "    text = \"<start>\" + user_input\n",
    "    number_of_strophes = len(form)\n",
    "    punctuation = ['.', ',', ':', ';', '!', '?', ' ']\n",
    "    verse_nb = 0\n",
    "    first_verse = True\n",
    "    input_length = syllable_count(user_input)\n",
    "\n",
    "    if(d):\n",
    "        special_gen_nb = 0\n",
    "\n",
    "    # Start generation of poem\n",
    "    for s in range(number_of_strophes): #Strophes\n",
    "\n",
    "        number_of_verses = form[s]\n",
    "    \n",
    "        for v in range(len(number_of_verses)): #Verses\n",
    "            verse_length = number_of_verses[v]\n",
    "            verse_nb_syllables = round(verse_length / 5)\n",
    "            verse_nb_tokens = round(verse_nb_syllables + ((verse_nb_syllables) / 2)) #Number of tokens in verse + one special token every two syllables\n",
    "            verse = \"\"\n",
    "            nb_generations = 0\n",
    "            special_gen = False\n",
    "            forbidden_tokens_special_gen = [[29],[9688],[27],[198],[220],[532],[60],[62],[26],[4210],[32],[960],[7],[27920], [685], [2361], [1391], [1782], [4808], [1279]] # tokens for \\n<start> = 198, 27, 9688, 29,\n",
    "\n",
    "            if not first_verse:\n",
    "                input_length = 0\n",
    "\n",
    "            max_lenght_verse = verse_nb_tokens + 2 + 4 - input_length\n",
    "            min_lenght_verse = verse_nb_tokens - 2 + 4 + round(len(text) / 4)- input_length\n",
    "\n",
    "            # Generate verse from the existing text\n",
    "            generated = \"\"\n",
    "            # Check if at least a full verse has been generated and if the verse has the correct number of syllables\n",
    "            while generated.count('<start>') < verse_nb + 2 or syllable_count_sentence(generated.split(\"<start>\")[verse_nb + 1]) != verse_nb_syllables:\n",
    "                \n",
    "                if nb_generations < 10: # Generate another verse up to 10 times\n",
    "                    generated = models[0](text, temperature = temperature, max_new_tokens = max_lenght_verse, min_length = min_lenght_verse)[0]['generated_text']\n",
    "                    nb_generations += 1\n",
    "                \n",
    "                else: # If the verse has not been generated after 10 tries, generate a new verse token by token\n",
    "                    if(d):\n",
    "                        print(\"special gen\")\n",
    "                        special_gen_nb += 1\n",
    "\n",
    "                    special_gen = True \n",
    "                    s = input_length\n",
    "                    if(s < verse_nb_syllables) : # This is necessary because with the user input the verse could already have the correct number of syllables\n",
    "                        #This first generation take the original text as input\n",
    "                        generated = models[0](text, temperature = temperature, max_new_tokens = 1, bad_words_ids = forbidden_tokens_special_gen )[0]['generated_text']\n",
    "                    else:\n",
    "                        generated = text\n",
    "                    while s < verse_nb_syllables:\n",
    "                        #the remaining generations take the original text plus the new tokens as an input\n",
    "                        generated = models[0](generated, temperature = temperature, max_new_tokens = 1, bad_words_ids = forbidden_tokens_special_gen)[0]['generated_text']\n",
    "                        s = syllable_count_sentence(generated.split(\"<start>\")[verse_nb + 1])\n",
    "                    generated += \"\\n<start>\"\n",
    "                    \n",
    "                    \n",
    "                        \n",
    "            verse = generated.split(\"<start>\")[verse_nb + 1] #With this we separate the new verse form the rest of the text\n",
    "            \n",
    "            if special_gen:\n",
    "                verse = verse.replace(\"-\",\"\")\n",
    "                print('special gen')\n",
    "            if first_verse:\n",
    "                verse = verse.replace(user_input,\"\")\n",
    "                first_verse = False\n",
    "\n",
    "            if (d):\n",
    "                print(\"Nb gen :\" , nb_generations)\n",
    "                print(verse)\n",
    "                \n",
    "\n",
    "            if v == (len(number_of_verses) - 1):\n",
    "                text = text + verse\n",
    "            else:\n",
    "                 text = text + verse  + \"<start>\"\n",
    "            verse_nb += 1\n",
    "\n",
    "        text = text + \"\\n<start>\"\n",
    "    \n",
    "    text = text.replace(\"<start>\", \"\")\n",
    "    if(d):\n",
    "        print(\"Temperature:\" , temperature)\n",
    "        print(\"Generated poem : \\n\" , text)\n",
    "        print(\"Nb of special gen:\" , special_gen_nb)\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "    # Postprocessing.\n",
    "    text = postprocessing(text, punctuation)\n",
    "\n",
    "\n",
    "    # Print poem into file\n",
    "    if print_into_file:\n",
    "        print('ahbon')\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S_GEN\") #Windows compatible\n",
    "        with open('lesgo' + '.txt', 'a') as f:\n",
    "            print('lesgo')\n",
    "            f.write(text)\n",
    "\n",
    "    if d:\n",
    "        print(\"Final poem: \\n\" , text)\n",
    "        print('\\n')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-bridal",
   "metadata": {
    "id": "informational-bridal",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Rhyme Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-anthropology",
   "metadata": {
    "id": "daily-anthropology"
   },
   "outputs": [],
   "source": [
    "def get_rhyming_dictionary (path=RIME_PICKLE_FILE):\n",
    "    \"\"\"Dictionaries of {\"word\": [perfect rhyme], assonant rhyme},\n",
    "    and the inversions, to have 3 total dictionaries.\n",
    "    \"\"\"\n",
    "    pickle_in = open(path,\"rb\")\n",
    "    word2rhymes, cons_rhyme2words, assonant_rhyme2words = pickle.load(pickle_in)\n",
    "\n",
    "    return word2rhymes, cons_rhyme2words, assonant_rhyme2words\n",
    "\n",
    "\n",
    "def tokenize_poem (text):\n",
    "    \"\"\"\n",
    "    Tokenize poem into strophes and verses.\n",
    "    \"\"\"\n",
    "    structured = []\n",
    "    strophes = text.split('\\n\\n')\n",
    "    for s in strophes:\n",
    "        strophe = []\n",
    "        verses = s.split('\\n')\n",
    "        for v in verses:\n",
    "            strophe += [v]\n",
    "        structured += [strophe]\n",
    "    return structured\n",
    "\n",
    "def process_verse (verse):\n",
    "    \"\"\"\n",
    "    Given a verse, returns its final punctuation (if any), last word, and remainder\n",
    "    of verse without either of the two.\n",
    "    \"\"\"\n",
    "\n",
    "    punctuation = ['.', ',', ',', ':', ';', '!', '?', ' ', '-']\n",
    "\n",
    "    tok_verse = word_tokenize(verse, language='english') #changed to english -Teo\n",
    "    # Remember punctuation\n",
    "    if tok_verse[-1] in punctuation:\n",
    "        original_punctuation = [tok_verse[-1]]\n",
    "    else:\n",
    "        original_punctuation = ''\n",
    "\n",
    "    # Get last word\n",
    "    if tok_verse[-1] in punctuation:\n",
    "        if len(tok_verse) > 1:\n",
    "            last_word = tok_verse [-2]\n",
    "        else:\n",
    "            last_word = ''\n",
    "    else:\n",
    "        last_word = tok_verse [-1]\n",
    "    if \"'\" in last_word:\n",
    "        last_word = last_word.split(\"'\")[1]\n",
    "\n",
    "    # Get new verse\n",
    "    try:\n",
    "        while tok_verse[-1] in punctuation:\n",
    "            tok_verse = tok_verse[:-1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    new_verse = ' '.join(tok_verse[:-1])\n",
    "    new_verse = re.sub (r\" â€™ \",r'â€™', new_verse)\n",
    "    new_verse = re.sub (r\" \\.\", '.', new_verse)\n",
    "\n",
    "    return original_punctuation, last_word, new_verse\n",
    "\n",
    "def get_candidates_rime (word_to_replace, word, dictios):\n",
    "    \"\"\"\n",
    "    Given a word and an dictionary, checks if word is in dictionary. If not, checks for closest.\n",
    "    If yes/with closest word: returns its consonant and assonant rhymes.\n",
    "    If there are consonant rhymes candidates, returns them, otherwise, assonant.\n",
    "    If none, returns an empty list.\n",
    "    \"\"\"\n",
    "    \n",
    "    word2rhymes, cons_rhyme2words, assonant_rhyme2words = dictios\n",
    "    \n",
    "    word = word.lower()\n",
    "    if word not in word2rhymes:\n",
    "        # find similar words\n",
    "        try:\n",
    "            word = difflib.get_close_matches(word, list(word2rhymes.keys()))[0]\n",
    "        except:\n",
    "            word = difflib.get_close_matches(word, list(word2rhymes.keys()), cutoff=0.25)[0]\n",
    "        \n",
    "    cons_rhyme = word2rhymes[word][0]\n",
    "    cons_rhyme_candidates = [w for w in cons_rhyme2words[cons_rhyme] if w != word]\n",
    "    # check if there are words that have consonant rhyme\n",
    "    if len(cons_rhyme_candidates) != 0:\n",
    "        rhyme = cons_rhyme\n",
    "        candidates = cons_rhyme_candidates\n",
    "    # if not, check if there are words that have assonant rhyme\n",
    "    else:\n",
    "        assonant_rhyme = word2rhymes[word][1]\n",
    "        assonant_rhyme_candidates = [w for w in assonant_rhyme2words[assonant_rhyme] if w != word]\n",
    "        if len(assonant_rhyme_candidates) != 0:\n",
    "            rhyme = assonant_rhyme\n",
    "            candidates = assonant_rhyme_candidates\n",
    "        # if not, return original word\n",
    "        else:\n",
    "            rhyme = \"\"\n",
    "            candidates = []\n",
    "    \n",
    "    return rhyme, candidates\n",
    "\n",
    "\n",
    "def seq_prob (verse_idx, strophe, old_verse, candidate, models):\n",
    "    \"\"\"\n",
    "    Calculates probability of a sequence of old_verse + ending (candidate) word.\n",
    "    \"\"\"\n",
    "    \n",
    "    #candidate = candidate.to_numpy()\n",
    "    candidate_in_context = []\n",
    "    for i in range(len(strophe)):\n",
    "        candidate_in_context.append(\"<start>\" + strophe[i])\n",
    "    candidate_in_context[verse_idx] = \"<start>\" + old_verse + \" \" + candidate\n",
    "\n",
    "    candidate_in_context = \"\\n\".join(candidate_in_context)\n",
    "\n",
    "    \n",
    "    tokenize_input = models[2].tokenize(candidate_in_context)\n",
    "    tensor_input = torch.tensor([models[2].convert_tokens_to_ids(tokenize_input)])\n",
    "    begin = time.time() #START ===============================================\n",
    "    loss = models[1](tensor_input, labels=tensor_input)\n",
    "    end = time.time()\n",
    "    print(f\"Total runtime of the model program is {end - begin}\") #END ===========\n",
    "    return -loss[0].item()\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def seq_prob (verse_idx, strophe, old_verse, candidate, models):\n",
    "  candidate_in_context = []\n",
    "  for i in range(len(strophe)):\n",
    "    candidate_in_context.append(\"<start>\" + strophe[i])\n",
    "  candidate_in_context[verse_idx] = \"<start>\" + old_verse + \" \" + candidate\n",
    "  candidate_in_context = \"\\n\".join(candidate_in_context)\n",
    "  tokenize_input = models[2].tokenize(candidate_in_context)\n",
    "  tensor_input = torch.tensor([models[2].convert_tokens_to_ids(tokenize_input)])\n",
    "  loss = models[1](tensor_input, labels=tensor_input)\n",
    "  return -loss[0].item()\n",
    "\n",
    "\n",
    "def apply_rhyming_scheme (poem,\n",
    "                          models,\n",
    "                          rhyming_scheme,\n",
    "                          outfile_directory=OUTPUT_DIRECTORY_FILE,\n",
    "                          print_into_file=False,\n",
    "                          d=False,\n",
    "                          n_candidates=30):\n",
    "    \"\"\"\n",
    "    Àlex: documentation is not up to date.\n",
    "    Gets a poem raw (as the first stage generates it), and according to the rhyming scheme\n",
    "    (e.g., ABBA ABBA CCD EDE, etc. [which is now missing]), returns it rhyming.\n",
    "    n_candidates = for how many candidate words are we going to calculate their probability.\n",
    "    rime_syllables = number of syllables that should rhyme.\n",
    "    print_into_file is used to DEBUG only (save to disk before and after rhyme)\n",
    "    Morever, 'd' allows printing of detailed debug info in TERMINAL\n",
    "    \"\"\"\n",
    "    \n",
    "    punctuation = ['.', ',', ',', ':', ';', '!', '?', ' ', '-']\n",
    "    tok_poem = tokenize_poem(poem) # segment poem into stanzas and verses.\n",
    "\n",
    "    new_poem = []\n",
    "    new_poem_markup = []\n",
    "    used_candidates = []\n",
    "    strophes_scheme = rhyming_scheme.split(' ')\n",
    "    \n",
    "    rhyming_dictios = get_rhyming_dictionary()\n",
    "\n",
    "    all_rhymes = dict([(c, '') for c in set(rhyming_scheme) if not c == ' '])\n",
    "\n",
    "    for strophe_idx in range(len(tok_poem)):\n",
    "        strophe = tok_poem[strophe_idx]\n",
    "        new_strophe = []\n",
    "        new_strophe_markup =[]\n",
    "\n",
    "        begin = time.time() #START ===============================================\n",
    "\n",
    "        for verse_idx in range(len(strophe)):\n",
    "            full_old_verse = strophe[verse_idx]\n",
    "            all_probs = []\n",
    "            punct, word, old_verse = process_verse(full_old_verse)\n",
    "            verse_scheme = strophes_scheme[strophe_idx][verse_idx]\n",
    "            if d: \n",
    "                print('\\tStanza: {} Verse: {} Rhyme: {}'.format(strophe_idx+1, verse_idx+1, verse_scheme))\n",
    "                print(all_rhymes[verse_scheme])\n",
    "\n",
    "            if all_rhymes[verse_scheme] == '':  # If it is a new rhyme leave it as is\n",
    "                all_rhymes[verse_scheme] = word\n",
    "                final_word = word\n",
    "                final_word_markup = word\n",
    "                rime = 'nochange'\n",
    "            else: # If it is not a new rhyme\n",
    "                rime, candidates = get_candidates_rime (word, all_rhymes[verse_scheme], rhyming_dictios)\n",
    "                \n",
    "                if d: print(\"#1a\", candidates[:100])\n",
    "                candidates = [i for i in candidates if i not in used_candidates and len(i) > 1]\n",
    "\n",
    "                if d: print(\"#1b\", candidates[:100])\n",
    "\n",
    "                if len(candidates) == 0:\n",
    "                    candidates = [word]                       \n",
    "\n",
    "                if len(candidates) < n_candidates:\n",
    "                    n_candidates = len(candidates)\n",
    "\n",
    "                random.shuffle(candidates)\n",
    "                random_candidates = random.sample(candidates, n_candidates)\n",
    "\n",
    "\n",
    "\n",
    "                begin = time.time() #START ===============================================\n",
    "\n",
    "                all_probs = []\n",
    "                for cand in random_candidates:\n",
    "                    all_probs.append(seq_prob(verse_idx, strophe, old_verse, cand, models).result())\n",
    "\n",
    "\n",
    "\n",
    "                end = time.time()\n",
    "                print(f\"Total runtime of the for parallel program is {end - begin}\") #END ===========\n",
    "                \n",
    "                begin = time.time() #START ===============================================\n",
    "                if d: \n",
    "                    print('\\t\\tALL CANDIDATES:', len(candidates), '\\n')\n",
    "                    if len(candidates) < 100:\n",
    "                        print('\\t\\t', word, rime, candidates, '\\n')\n",
    "                \n",
    "                #all_probs = rand_cand.apply(lambda cand: seq_prob(verse_idx, strophe, old_verse, cand, models))\n",
    "                \n",
    "                \n",
    "                #for candidate in random_candidates: #pndidatesararalelize random ca\n",
    "                    #prob = seq_prob (verse_idx, strophe, old_verse, candidate, models) \n",
    "                    # vu l'appel de apply_rhyming_scheme\n",
    "                    # on n'aurait ici que la pipeline avec les modÃ¨les\n",
    "                    #all_probs += [prob]\n",
    "\n",
    "                end = time.time()\n",
    "                print(f\"Total runtime of the for candidates program is {end - begin}\") #END ===========\n",
    "\n",
    "                begin = time.time() #START ===============================================\n",
    "                print(all_probs)\n",
    "                highest_to_lowest = np.argsort(all_probs)[::-1] # why sort all if we only need max?\n",
    "                end = time.time()\n",
    "                print(f\"Total runtime of the argsort program is {end - begin}\") #END ===========\n",
    "\n",
    "                best = highest_to_lowest[0]\n",
    "                best_prob = all_probs[best]\n",
    "                final_word = random_candidates[best]\n",
    "                final_word_markup = \"[u]\" + final_word + \"[/u]\"\n",
    "\n",
    "            if len(punct) != 0:\n",
    "                best_riming_verse = old_verse + ' ' + ''.join(final_word) + punct[0]\n",
    "                best_riming_verse_markup = old_verse + ' ' + ''.join(final_word_markup) + punct[0]\n",
    "            else:\n",
    "                best_riming_verse = old_verse + ' ' + ''.join(final_word)\n",
    "                best_riming_verse_markup = old_verse + ' ' + ''.join(final_word_markup)\n",
    "\n",
    "            used_candidates += [final_word]\n",
    "\n",
    "            best_riming_verse += '\\n'\n",
    "            best_riming_verse_markup += '\\n'\n",
    "\n",
    "            new_strophe += [best_riming_verse]\n",
    "            new_strophe_markup += [best_riming_verse_markup]\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Total runtime of the chargin program is {end - begin}\") #END ===========\n",
    "\n",
    "        new_strophe += ['\\n']\n",
    "        new_strophe_markup += ['\\n']\n",
    "\n",
    "        new_poem += [new_strophe]\n",
    "        new_poem_markup += [new_strophe_markup]\n",
    "\n",
    "    raw = ''.join((flatten(new_poem)))\n",
    "    new_poem = postprocessing(raw, punctuation)\n",
    "\n",
    "    raw_markup = ''.join((flatten(new_poem_markup)))\n",
    "    new_poem_markup = postprocessing(raw_markup, punctuation)\n",
    "\n",
    "    if d: \n",
    "        print('NEW POEM\\n\\n' + new_poem + '\\n' + '~'*50 + '\\n' + 'OLD POEM\\n\\n' + poem)\n",
    "    if print_into_file:\n",
    "        now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S_RHYME\") #Windows compatible\n",
    "        with open('10Mo' + '.txt', 'a') as f:\n",
    "            #f.write('NEW POEM\\n\\n' + new_poem + '\\n' + '~'*50 + '\\n' + 'OLD POEM\\n\\n' + poem)\n",
    "            f.write(new_poem )\n",
    "        with open('/content/drive/MyDrive/crpo/' + '10Mo' + '.txt', 'a') as f:\n",
    "            #f.write('NEW POEM\\n\\n' + new_poem + '\\n' + '~'*50 + '\\n' + 'OLD POEM\\n\\n' + poem)\n",
    "            f.write(new_poem )\n",
    "\n",
    "    return (new_poem_markup, new_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-methodology",
   "metadata": {
    "id": "engaging-methodology"
   },
   "source": [
    "## 5. Function Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-spirit",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "familiar-spirit",
    "outputId": "f0d217c1-ca6b-4e94-8416-13981aac075a"
   },
   "outputs": [],
   "source": [
    "begin = time.time()\n",
    "MODEL = load_models (['gpt2-poetry-model-with-start-token_100_epochs'], \"/content/drive/Shareddrives/ML4Science/\")[0] #CHANGE HERE FOR NEW MODEL\n",
    "end = time.time()\n",
    "print(f\"Total runtime of the loading is {end - begin}\")\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    non_rhyming_poem = generate(QUATRAIN,\n",
    "                                MODEL,\n",
    "                                OUTPUT_DIRECTORY_FILE,\n",
    "                                temperature= 1.0,\n",
    "                                print_into_file=False,\n",
    "                                d=False,\n",
    "                                user_input='')\n",
    "    #print('===================================================')\n",
    "    print('Here goes the poem generated : ')\n",
    "    #print('===================================================')\n",
    "\n",
    "    #print(non_rhyming_poem) #now it should rhyme\n",
    "    with open('outputs.txt', 'a', encoding='utf-8') as w:\n",
    "        w.write(non_rhyming_poem)\n",
    "    \n",
    "    \n",
    "    #begin = time.time()\n",
    "    '''\n",
    "    rhyming_poem_markup, rhyming_poem = apply_rhyming_scheme (non_rhyming_poem,\n",
    "                                                              MODEL,\n",
    "                                                              RHYMING_SCHEME,\n",
    "                                                              outfile_directory=OUTPUT_DIRECTORY_FILE,\n",
    "                                                              print_into_file=True,\n",
    "                                                              d=False,\n",
    "                                                              n_candidates=30)\n",
    "    '''\n",
    "    #print(rhyming_poem)\n",
    "    #end = time.time()\n",
    "    #print(f\"Total runtime of the program is {end - begin}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
